---
title: "Climate Modeling"
description: "Climate modeling uses mathematical simulations of the Earth's climate system to project future temperature, precipitation, and extreme weather patterns."
category: "Climate Resilience"
relatedTerms: ["climate-vulnerability-assessment", "climate-tipping-points", "greenhouse-gas-emissions", "climate-feedback-loops"]
lastUpdated: "2026-02-22"
---

## What is Climate Modeling?

Climate modeling is the use of mathematical representations of the Earth's atmosphere, oceans, land surface, and ice systems to simulate climate behavior under different conditions. General Circulation Models (GCMs) — now increasingly called Earth System Models (ESMs) — divide the planet into three-dimensional grid cells and solve physics equations governing energy, mass, and momentum transfer at each cell. The Coupled Model Intercomparison Project (CMIP) coordinates global modeling efforts; CMIP6, the latest generation, includes output from over 100 models produced by 50+ research institutions worldwide.

## Why It Matters

Every climate risk assessment, adaptation plan, and disclosure framework relies on climate model output. Without modeling, organizations have no basis for projecting how physical risks will evolve over the 10–50 year horizons relevant to infrastructure investment, real estate portfolios, and strategic planning. The models underlying IPCC projections inform trillions of dollars in capital allocation decisions.

The regulatory demand for scenario analysis has made climate modeling operationally relevant for mainstream business. TCFD, ISSB, and CSRD all require organizations to assess climate risks under multiple scenarios — typically 1.5°C, 2°C, and 3°C+ warming pathways. These scenarios are derived directly from climate model ensembles. Financial regulators including the Bank of England, European Central Bank, and Federal Reserve use NGFS climate scenarios (built on model output) for stress testing.

Model resolution and capability have advanced dramatically. CMIP6 models run at 50–100 km resolution, capturing large-scale circulation patterns but missing local features. High-resolution models (1–10 km) can resolve individual thunderstorms, urban heat islands, and coastal processes. Machine learning is accelerating both model development and downscaling — Google DeepMind's GraphCast weather model produces 10-day forecasts in under a minute that match or exceed the accuracy of physics-based models that take hours on supercomputers.

Understanding model capabilities and limitations is essential for responsible use. Models are tools, not crystal balls. They project ranges of possible futures conditional on emission scenarios, not predictions of what will happen. Communicating uncertainty honestly — including the tails of probability distributions where tipping points and surprises live — is as important as communicating central estimates.

## How It Works / Key Components

GCMs represent the atmosphere, ocean, land surface, and cryosphere as interconnected systems. Atmospheric models solve the Navier-Stokes equations for fluid dynamics, radiation transfer, and thermodynamics on a global grid. Ocean models simulate circulation, heat transport, and biogeochemistry. Land surface models represent vegetation, soil moisture, carbon cycling, and hydrology. Ice models simulate glacier and ice sheet dynamics. The models exchange energy, moisture, and momentum at boundaries — the atmosphere drives ocean surface winds and precipitation, the ocean provides heat and moisture to the atmosphere.

Emission scenarios provide the forcing input. The Shared Socioeconomic Pathways (SSPs) used in CMIP6 combine socioeconomic development trajectories with greenhouse gas concentration pathways. SSP1-2.6 represents strong mitigation consistent with ~1.5–2°C warming. SSP2-4.5 represents moderate mitigation (~2.1–3.5°C). SSP5-8.5 represents very high emissions (~3.3–5.7°C). Running models under multiple SSPs produces the envelope of possible futures that organizations use for scenario analysis.

Downscaling bridges the gap between global model resolution and local decision needs. Statistical downscaling uses historical relationships between large-scale climate patterns and local conditions to refine GCM output to 1–10 km resolution. Dynamical downscaling nests high-resolution regional models within GCMs to simulate local processes explicitly. Both approaches introduce additional uncertainty. Platforms like WorldClim, CHELSA, and NASA Earth Exchange provide downscaled datasets that planners and analysts can apply directly.

Model validation and ensemble methods manage uncertainty. Individual models have biases — some run too hot, others too cold, some overestimate rainfall, others underestimate it. Using multi-model ensembles (typically 20–40 models) averages out individual model biases and provides a range of outcomes. Historical validation confirms that models accurately reproduce observed 20th-century warming, precipitation trends, and extreme event statistics — building confidence in their future projections.

## Climate Modeling in Practice

The New York City Panel on Climate Change (NPCC) uses an ensemble of 35+ CMIP6 models, downscaled to the metropolitan area, to produce probabilistic projections for temperature, precipitation, and sea-level rise that inform all city infrastructure planning. The projections include explicit uncertainty ranges — for example, sea-level rise of 8–30 inches by 2050 (10th to 90th percentile) — ensuring that planners design for a range of futures rather than a single number.

In the private sector, Jupiter Intelligence combines climate model output with machine learning to provide asset-level risk scoring for real estate, infrastructure, and agricultural portfolios. Their platform translates model projections into business-relevant metrics: flood depth at a specific building, heat stress days at a particular worksite, wildfire probability for a given parcel.

## Council Fire's Approach

Council Fire translates climate model output into actionable strategy for organizations navigating physical climate risk. We specialize in interpreting model projections for coastal and marine systems — where ocean-atmosphere interactions, storm surge modeling, and marine ecosystem responses require specialized expertise beyond standard terrestrial climate analysis. Our approach pairs technical rigor with clear communication, helping boards and stakeholders understand what models tell us, what they don't, and how to make decisions under uncertainty.

## Frequently Asked Questions

### How accurate are climate models?
Climate models have successfully projected global temperature trends within observed ranges for decades. Hansen's 1988 projections tracked actual warming remarkably well. Models accurately reproduce observed patterns of warming (more at poles than tropics, more at night than day, more over land than ocean). Regional projections carry more uncertainty, particularly for precipitation. The key insight: models are most reliable for large-scale, long-term trends and least reliable for local, short-term variability. Using multi-model ensembles substantially improves reliability.

### What's the difference between weather forecasting and climate modeling?
Weather models predict specific atmospheric conditions days ahead — will it rain in Chicago on Tuesday? Climate models project statistical properties of weather over decades — how will average rainfall and extreme precipitation frequency change in the Midwest by 2060? Weather forecasting loses accuracy beyond ~10 days because of chaotic atmospheric dynamics. Climate modeling works at longer timescales precisely because it's projecting averages and distributions, not specific events. Different questions, different tools.

### How should non-scientists interpret climate model uncertainty?
Treat model uncertainty as a range of plausible outcomes, not as evidence that we don't know anything. When models project 1.5–4.5°C of warming under a given scenario, this means all outcomes in that range are physically consistent with our understanding. Decision-makers should plan for the range — designing for the median while stress-testing against the extremes. The uncertainty itself is information: wider ranges for some variables (like precipitation) signal where more conservative planning assumptions are warranted.
